{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import torch\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "def load_pdf_files(files):\n",
    "    # Assuming the provided 'files' list contains URLs\n",
    "    pdf_url = files[0]\n",
    "\n",
    "    # Read PDF content from the URL\n",
    "    pdf_loader = PyPDFLoader(pdf_url)\n",
    "    docs = pdf_loader.load_and_split()\n",
    "\n",
    "    # If there are more files in the list, process them\n",
    "    for i in range(1, len(files)):\n",
    "        pdf_url = files[i]\n",
    "        pdf_loader = PyPDFLoader(pdf_url)\n",
    "        docs += pdf_loader.load_and_split()\n",
    "\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs in PDF files:  187\n"
     ]
    }
   ],
   "source": [
    "files=[\"./DataSourceFiles/IPC_186045.pdf\"]\n",
    "# files=[\"D:/Files/LLM/Project/DataSourceFiles/IPC_186045.pdf\",\"D:\\Files\\LLM\\Src_docs\\special_marriage_act.pdf\",\"D:\\Files\\LLM\\Src_docs\\THE_CODE_OF_CIVIL_PROCEDURE_1908.pdf\",\"D:\\Files\\LLM\\Src_docs\\THE_LAND_ACQUISITION_ACT.pdf\"]\n",
    "docs=load_pdf_files(files)\n",
    "print(\"docs in PDF files: \",len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# def split_docs(documents,chunk_size=1000,chunk_overlap=20):\n",
    "#   text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "#   docs = text_splitter.split_documents(documents)\n",
    "#   return docs\n",
    "\n",
    "# splited_docs = split_docs(docs)\n",
    "# print(len(splited_docs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Files\\LLM\\Project\\venv\\lib\\site-packages\\InstructorEmbedding\\instructor.py:7: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n",
      "vectore store created\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langchain.vectorstores.faiss.FAISS at 0x1b119b73b20>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import  HuggingFaceInstructEmbeddings,HuggingFaceHubEmbeddings,HuggingFaceEmbeddings\n",
    "\n",
    "HFIembeddings = HuggingFaceInstructEmbeddings(model_name=\"thenlper/gte-small\",cache_folder=\"./Models/\", model_kwargs={\"device\": DEVICE})\n",
    "# HFIembeddings = HuggingFaceInstructEmbeddings(model_name=\"WhereIsAI/UAE-Large-V1\")    \n",
    "# HFIembeddings = HuggingFaceHubEmbeddings(repo_id=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "# HFIembeddings = HuggingFaceHubEmbeddings(repo_id=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "# HFIembeddings = HuggingFaceEmbeddings(model_name=\"D:\\Files\\LLM\\Project\\Models\\sentence-transformers_all-MiniLM-L6-v2\",cache_folder=\"./Models/\")\n",
    "vectorstore = FAISS.from_documents(docs,embedding=HFIembeddings)\n",
    "print(\"vectore store created\")\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "# embeddings = SentenceTransformerEmbeddings(\n",
    "#     model_name=\"D:\\Files\\LLM\\Project\\Models\\sentence-transformers_all-MiniLM-L6-v2\",\n",
    "#     model_kwargs={'device': 'cuda'},\n",
    "#     encode_kwargs={'normalize_embeddings': True},\n",
    "#     # query_instruction=\"Represent this sentence for searching relevant passages: \"\n",
    "# )\n",
    "\n",
    "# from langchain.vectorstores import Chroma\n",
    "# vectorstore = Chroma.from_documents(splited_docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.vectorstores import Chroma\n",
    "# # from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "# # from langchain.embeddings import  HuggingFaceInstructEmbeddings,HuggingFaceHubEmbeddings,HuggingFaceEmbeddings\n",
    "\n",
    "# from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# # model = SentenceTransformer(model_name_or_path=\"sentence-transformers/all-MiniLM-L6-v2\",cache_folder=\"./Models/\")\n",
    "# sentence_transformer_ef = SentenceTransformerEmbeddingFunction(model_name=\"D:\\Files\\LLM\\Project\\Models\\sentence-transformers_all-MiniLM-L6-v2\")\n",
    "# # # HFIembeddings = HuggingFaceHubEmbeddings(repo_id=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "# # # # embedding_function = SentenceTransformerEmbeddings(model_name=\"gte-small\") #,cache_folder=\"./Models/\")\n",
    "# # print((sentence_transformer_ef))\n",
    "# vectorstore = Chroma.from_documents(documents=pages,embedding=sentence_transformer_ef,collection_name=\"law_collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "template = \"\"\"\n",
    "    - You're a helpful AI assistant assigned to assist individuals seeking legal advice within the framework of Indian laws and the constitution.\n",
    "    - Your role is to guide users through legal processes and provide information in a lawful manner.\n",
    "    - Use the given text to answer the question in atleast 1000 words, give all accurate information.\n",
    "    - Answer questions step by step, highlighting relevant sections of Indian laws and the constitution, use bulleting to display more pretty and readable answer.\n",
    "    - Refrain from responding to queries that may not contribute to legal affairs, and provide accurate and relevant information without distortion.\n",
    "    - deny to give answers if its not available into provided text.\n",
    "{context}\n",
    "\n",
    "Question: {question} \n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import HuggingFaceHub\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm = HuggingFaceHub(repo_id=\"mistralai/Mixtral-8x7B-Instruct-v0.1\", model_kwargs={\"temperature\":0.8, \"max_length\":4096,\"max_new_tokens\":4096}),\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(\n",
    "        search_type='similarity',\n",
    "        search_kwargs={\n",
    "            'k': 4,\n",
    "            'filter': {\"source\": files[0]}\n",
    "        },\n",
    "    ),\n",
    "    # return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt} #,\"verbose\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document './tmp/2024-02-11_10_08_50.docx' generated successfully.\n"
     ]
    }
   ],
   "source": [
    "# q = \"what is the punishment for robbery ?\"\n",
    "# q = \"who is judge ?\"\n",
    "q = \"one person has commited defamation, what should be punishment for it?\"\n",
    "# q = \"rioting has happened in neighbouring society. who will be responsible and what is the punishment for it\"\n",
    "# q = \"what is punishment for robbery and also murder ?\"\n",
    "# q = \"what are the rights of tenant and landlord ?\"\n",
    "# q = \"Liability of person for whose benefit riot is committed\"\n",
    "# q = \"person is selling adulterated drugs which is harmful to the health of people. person has also sold drug to the children below age of 15 what will be punishment for it?\"\n",
    "# q=\"start with python programming language\"\n",
    "# q=\"explain all laws related to merriage\"\n",
    "# q+=\" and what is a household work?\"\n",
    "response = chain({\"query\":q, \"early_stopping\":True,\"min_length\":2000,\"max_tokens\":5000})\n",
    "\n",
    "from write_in_file import generate_docx_with_bullets\n",
    "generate_docx_with_bullets(heading=q,main_paragraph=response[\"result\"],output_folder=\"./tmp/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def write_in_notepad(response):\n",
    "#     from write_in_file import open_in_notepad\n",
    "#     open_in_notepad(response[\"result\"])\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
